import pandas as pd
import numpy as np
import os
import glob

class DataCleaningPipeline:
    """
    Pipeline de ETL respons√°vel pela ingest√£o, limpeza e padroniza√ß√£o dos dados.
    Estrutura:
    1. Identifica arquivos ignorando prefixos (ex: [Duda]).
    2. Aplica regras de neg√≥cio (datas, nulos, imputa√ß√£o).
    3. Salva dados prontos na camada 'processed'.
    """

    def __init__(self, input_path: str = "/content/", output_path: str = "/content/data/processed/"):
        # Define caminhos de origem (Raw) e destino (Trusted/Processed)
        self.input_path = input_path
        self.output_path = output_path
        os.makedirs(self.output_path, exist_ok=True)

    def _load_file(self, suffix: str) -> pd.DataFrame:
        """
        Busca arquivos dinamicamente usando sufixo para ignorar nomes complexos.
        Ex: Procura por '*pedidos.csv' e acha '[Duda] DataLake - pedidos.csv'.
        """
        files = glob.glob(f"{self.input_path}*{suffix}")
        if not files:
            raise FileNotFoundError(f"Arquivo finalizado em {suffix} n√£o encontrado.")
        
        file_path = files[0]
        print(f"üìÇ Lendo: {os.path.basename(file_path)}")
        
        # Tenta carregar com UTF-8, fallback para Latin-1 (comum no Brasil)
        try:
            return pd.read_csv(file_path, encoding='utf-8')
        except UnicodeDecodeError:
            return pd.read_csv(file_path, encoding='latin-1')

    def _save_file(self, df: pd.DataFrame, filename: str):
        """Padroniza o salvamento na pasta processed."""
        path = f"{self.output_path}{filename}"
        df.to_csv(path, index=False)
        print(f"‚úÖ Salvo em: {path}")

    def process_orders(self):
        """
        Regra de Neg√≥cio: Pedidos.
        - Convers√£o de strings para datetime.
        - Mant√©m NaT (Not a Time) pois indica que o evento (ex: entrega) ainda n√£o ocorreu.
        """
        df = self._load_file("pedidos.csv")
        
        # Lista de colunas de data para convers√£o
        date_cols = ['order_purchase_timestamp', 'order_approved_at', 
                     'order_delivered_carrier_date', 'order_delivered_customer_date', 
                     'order_estimated_delivery_date']
        
        # 'coerce' transforma erros/nulos em NaT automaticamente
        for col in date_cols:
            df[col] = pd.to_datetime(df[col], errors='coerce')

        self._save_file(df, "pedidos_clean.csv")

    def process_products(self):
        """
        Regra de Neg√≥cio: Produtos.
        - Categoricos nulos -> 'outros'.
        - Contagens nulas -> 0.
        - Dimens√µes/Pesos nulos -> Mediana (robusto a outliers).
        """
        df = self._load_file("produtos.csv")

        # 1. Tratamento Categ√≥rico
        df['product_category_name'] = df['product_category_name'].fillna('outros')

        # 2. Tratamento Num√©rico (Contagens)
        cols_zero = ['product_name_lenght', 'product_description_lenght', 'product_photos_qty']
        df[cols_zero] = df[cols_zero].fillna(0)

        # 3. Tratamento Num√©rico (Dimens√µes F√≠sicas)
        cols_dims = ['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']
        for col in cols_dims:
            df[col] = df[col].fillna(df[col].median())

        self._save_file(df, "produtos_clean.csv")

    def process_others(self):
        """
        Regra de Neg√≥cio: Itens e Vendedores.
        - Move dados j√° limpos para a zona processed para manter consist√™ncia do Data Lake.
        """
        for file in ["itens_pedidos.csv", "vendedores.csv"]:
            try:
                df = self._load_file(file)
                # Adiciona prefixo clean_ para padronizar sa√≠da
                self._save_file(df, f"clean_{file}")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao processar {file}: {e}")

    def run(self):
        """M√©todo p√∫blico que orquestra todo o pipeline."""
        print("--- üöÄ Iniciando Pipeline de ETL ---")
        self.process_orders()
        self.process_products()
        self.process_others()
        print("--- ‚ú® Pipeline Finalizado com Sucesso ---")

# --- Ponto de Entrada (Execution) ---
if __name__ == "__main__":
    # Instancia e executa
    pipeline = DataCleaningPipeline()
    pipeline.run()